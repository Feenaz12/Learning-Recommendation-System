# -*- coding: utf-8 -*-
"""Preprocess.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RDOAat3UUEinaCRcc897cNHKliOHoMmW
"""

# Mount Google Drive (if files saved there)
from google.colab import drive
drive.mount('/content/drive')

# preprocess.py

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
import os

# Paths (update according to your Google Drive)
students_path = "/content/drive/MyDrive/Masai/MASAI-PROJECTS/LRS/students.csv"
courses_path = "/content/drive/MyDrive/Masai/MASAI-PROJECTS/LRS/courses.csv"
models_dir = "/content/drive/MyDrive/Masai/MASAI-PROJECTS/LRS/models/"

# Create models directory if it doesn't exist
os.makedirs(models_dir, exist_ok=True)

# 1. Load data
students = pd.read_csv(students_path)
courses = pd.read_csv(courses_path)

# 2. Combine relevant text fields for students
students['profile_text'] = students['skills'].fillna('') + " " + students['interests'].fillna('')

# 3. Combine course text fields
courses['course_text'] = courses['course_name'].fillna('') + " " + courses['category'].fillna('')

# 4. Fit TF-IDF Vectorizer on combined texts
all_texts = pd.concat([students['profile_text'], courses['course_text']], ignore_index=True)
vectorizer = TfidfVectorizer(stop_words='english', max_features=500)
vectorizer.fit(all_texts)

# 5. Transform separately
student_matrix = vectorizer.transform(students['profile_text'])
course_matrix = vectorizer.transform(courses['course_text'])

# 6. Save vectorizer & matrices
joblib.dump(vectorizer, os.path.join(models_dir, "vectorizer.pkl"))
joblib.dump(student_matrix, os.path.join(models_dir, "student_matrix.pkl"))
joblib.dump(course_matrix, os.path.join(models_dir, "course_matrix.pkl"))

print("Preprocessing completed and models saved successfully!")

